---
title: "Building w/ Hugging Face"
summary: "A quick guide on creating models with Hugging Face."
topicTitle: "Guides"
topicSlug: "guides"
nextTitle: "Loom AI Expanded"
nextSlug: "/guides/build-loom-ai"   
---

## Step 1: Install Necessary Libraries

Ensure you have the Hugging Face transformers library installed.

```bash
pip install transformers datasets torch
```

## Step 2: Load a Pre-Trained Model

Hugging Face provides multiple pre-trained models. Below is an example using `bert-base-uncased`:

```bash
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
```

## Step 3: Tokenize and Prepare Data

Convert raw text into tokenized inputs that can be processed by the model:

```bash
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)
```

## Step 4: Load dataset

Hugging Face datasets allow easy loading and preprocessing:

```bash
from datasets import load_dataset
dataset = load_dataset("imdb")  # Example dataset
```

## Step 5: Fine-Tune the Model

Use Hugging Faceâ€™s `Trainer` API for easy model training:

```bash
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"]
)

trainer.train()
```

## Step 6: Save and Deploy the Model

After training, save and deploy the model:

```bash
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")
```